/*
 * SmartHopper - AI-powered Grasshopper Plugin
 * Copyright (C) 2025 Marc Roca Musach
 *
 * This library is free software; you can redistribute it and/or
 * modify it under the terms of the GNU Lesser General Public
 * License as published by the Free Software Foundation; either
 * version 3 of the License, or (at your option) any later version.
 */

using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.Globalization;
using System.Linq;
using System.Threading.Tasks;
using Newtonsoft.Json.Linq;
using SmartHopper.Core.Grasshopper.Utils.Parsing;
using SmartHopper.Infrastructure.AICall.Core.Base;
using SmartHopper.Infrastructure.AICall.Core.Interactions;
using SmartHopper.Infrastructure.AICall.Core.Requests;
using SmartHopper.Infrastructure.AICall.Core.Returns;
using SmartHopper.Infrastructure.AICall.Metrics;
using SmartHopper.Infrastructure.AICall.Tools;
using SmartHopper.Infrastructure.AIModels;
using SmartHopper.Infrastructure.AITools;

namespace SmartHopper.Core.Grasshopper.AITools
{
    /// <summary>
    /// Contains tools for list generation using AI.
    /// </summary>
    public class list_generate : IAIToolProvider
    {
        /// <summary>
        /// Name of the AI tool provided by this class.
        /// </summary>
        private readonly string toolName = "list_generate";

        /// <summary>
        /// Minimum batch size for chunked requests.
        /// </summary>
        private const int MinBatchSize = 10;

        /// <summary>
        /// Minimum batch size when further reducing after truncation.
        /// </summary>
        private const int MinReducedBatchSize = 3;

        /// <summary>
        /// Default batch size divisor (e.g., count / 3).
        /// </summary>
        private const int DefaultBatchDivisor = 3;

        /// <summary>
        /// Default timeout for AI requests.
        /// </summary>
        private const int DefaultTimeoutSeconds = 300;

        /// <summary>
        /// Defines the required capabilities for the AI tool provided by this class.
        /// </summary>
        private readonly AICapability toolCapabilityRequirements = AICapability.TextInput | AICapability.JsonOutput;

        /// <summary>
        /// JSON schema for list output.
        /// </summary>
        private readonly string listJsonSchema = @"{
            ""type"": ""array"",
            ""description"": ""An array of strings generated by the assistant."",
            ""items"": {
                ""type"": ""string"",
                ""description"": ""List item as a string.""
            }
        }";

        /// <summary>
        /// System prompt for the AI tool provided by this class.
        /// </summary>
        private readonly string systemPrompt =
            "You are a list generator assistant. Your task is to generate a specific number of items based on the user's prompt and return them as a JSON array.\n\n" +
            "IMPORTANT REQUIREMENTS:\n" +
            "- Return ONLY a valid JSON array of strings\n" +
            "- Each item must be a quoted string, even if it contains commas or special characters\n" +
            "- Generate exactly the requested number of items\n" +
            "- Do not include any extra text, explanations, or formatting\n" +
            "- Do not wrap the output in code blocks or additional quotes\n\n" +
            "OUTPUT EXAMPLES:\n" +
            "['item1', 'item2', 'item3']\n" +
            "['{1,0,0}', '{0.707,0.707,0}', '{0,1,0}']\n" +
            "['apple', 'banana with, comma', 'orange']";

        /// <summary>
        /// User prompt for the AI tool provided by this class. Use <prompt> and <count> placeholders.
        /// </summary>
        private readonly string userPrompt =
            "Generate exactly <count> items based on this prompt: \"<prompt>\"\n\n" +
            "Return only the JSON array of strings.";

        /// <summary>
        /// Get all tools provided by this class.
        /// </summary>
        /// <returns>An enumerable collection of AI tools provided by this class.</returns>
        public IEnumerable<AITool> GetTools()
        {
            yield return new AITool(
                name: this.toolName,
                description: "Generates a list of items based on a prompt, count and type",
                category: "DataProcessing",
                parametersSchema: @"{
                    ""type"": ""object"",
                    ""properties"": {
                        ""prompt"": { ""type"": ""string"", ""description"": ""The prompt to generate items from"" },
                        ""count"": { ""type"": ""integer"", ""description"": ""Number of items to generate"" },
                        ""type"": { ""type"": ""string"", ""description"": ""Type of items (e.g. 'text', 'number', 'integer', 'boolean')"", ""enum"": [""text"", ""number"", ""integer"", ""boolean""] }
                    },
                    ""required"": [""prompt"", ""count"", ""type""]
                }",
                execute: this.GenerateList,
                requiredCapabilities: this.toolCapabilityRequirements);
        }

        /// <summary>
        /// Unwraps a JSON response that may be wrapped in an object with 'items' or 'list' property.
        /// </summary>
        /// <param name="response">The response string to unwrap.</param>
        /// <returns>Unwrapped response string.</returns>
        private static string UnwrapResponseIfNeeded(string response)
        {
            if (string.IsNullOrWhiteSpace(response))
            {
                return response;
            }

            try
            {
                var trimmed = response.TrimStart();
                if (trimmed.StartsWith("{"))
                {
                    var obj = JObject.Parse(response);
                    var itemsArray = (obj["items"] as JArray) ?? (obj["list"] as JArray);
                    if (itemsArray != null)
                    {
                        return itemsArray.ToString(Newtonsoft.Json.Formatting.None);
                    }
                }
            }
            catch (Exception ex)
            {
                Debug.WriteLine($"[ListTools] Could not unwrap response: {ex.Message}");
            }

            return response;
        }

        /// <summary>
        /// Trims the list to the exact count if it exceeds it.
        /// </summary>
        /// <param name="items">List to trim.</param>
        /// <param name="targetCount">Target count.</param>
        private static void TrimToCount(List<string> items, int targetCount)
        {
            if (items.Count > targetCount)
            {
                items.RemoveRange(targetCount, items.Count - targetCount);
            }
        }

        /// <summary>
        /// Creates a tool result JObject with envelope.
        /// </summary>
        /// <param name="items">List items to include.</param>
        /// <param name="toolName">Tool name.</param>
        /// <param name="providerName">Provider name.</param>
        /// <param name="modelName">Model name.</param>
        /// <param name="toolCallId">Tool call ID.</param>
        /// <returns>JObject with envelope.</returns>
        private JObject CreateToolResultWithEnvelope(List<string> items, string providerName, string modelName, string? toolCallId)
        {
            var toolResult = new JObject();
            toolResult.Add("list", new JArray(items));

            toolResult.WithEnvelope(
                ToolResultEnvelope.Create(
                    tool: this.toolName,
                    type: ToolResultContentType.List,
                    payloadPath: "list",
                    provider: providerName,
                    model: modelName,
                    toolCallId: toolCallId));

            return toolResult;
        }

        /// <summary>
        /// Calculates a safe batch size based on received items and target count.
        /// </summary>
        /// <param name="receivedCount">Number of items received.</param>
        /// <param name="targetCount">Total target count.</param>
        /// <returns>Calculated batch size.</returns>
        private static int CalculateBatchSize(int receivedCount, int targetCount)
        {
            return receivedCount > 0
                ? Math.Max(MinBatchSize, receivedCount)
                : Math.Max(MinBatchSize, targetCount / DefaultBatchDivisor);
        }

        /// <summary>
        /// Tool wrapper for the GenerateList function.
        /// </summary>
        /// <param name="parameters">Parameters passed from the AI.</param>
        /// <returns>Result object.</returns>
        private async Task<AIReturn> GenerateList(AIToolCall toolCall)
        {
            // Prepare the output
            var output = new AIReturn()
            {
                Request = toolCall,
            };

            try
            {
                Debug.WriteLine("[ListTools] Running GenerateList tool");

                // Extract parameters
                string providerName = toolCall.Provider;
                string modelName = toolCall.Model;
                string endpoint = this.toolName;
                AIInteractionToolCall toolInfo = toolCall.GetToolCall();
                var args = toolInfo.Arguments ?? new JObject();
                string? prompt = args["prompt"]?.ToString();
                int count = args["count"]?.ToObject<int>() ?? 0;
                string? type = args["type"]?.ToString();
                string? contextFilter = args["contextFilter"]?.ToString() ?? string.Empty;

                if (string.IsNullOrEmpty(prompt) || count <= 0 || string.IsNullOrEmpty(type))
                {
                    output.CreateError("Missing or invalid parameters: prompt, count, or type");
                    return output;
                }

                if (!type.Equals("text", StringComparison.OrdinalIgnoreCase))
                {
                    output.CreateError($"Type '{type}' not supported");
                    return output;
                }

                // Use iterative approach to ensure we get the exact count with conversational logic
                var allItems = new List<string>();
                const int maxIterations = 10;
                int iteration = 0;
                int currentBatchSize = count; // Start with full count, will adjust if truncated
                AIReturn? result = null;

                // Accumulate metrics across all iterations
                var accumulatedMetrics = new AIMetrics();
                var allMessages = new List<AIRuntimeMessage>();

                // 1. Generate initial request
                var initialUserPrompt = this.userPrompt;
                initialUserPrompt = initialUserPrompt.Replace("<prompt>", prompt);
                initialUserPrompt = initialUserPrompt.Replace("<count>", count.ToString(CultureInfo.InvariantCulture));

                // Initiate immutable AIBody
                var requestBody = AIBodyBuilder.Create()
                    .WithJsonOutputSchema(this.listJsonSchema)
                    .AddSystem(this.systemPrompt)
                    .AddUser(initialUserPrompt)
                    .WithContextFilter(contextFilter)
                    .Build();

                // Initiate AIRequestCall
                var request = new AIRequestCall();
                request.Initialize(
                    provider: providerName,
                    model: modelName,
                    capability: this.toolCapabilityRequirements,
                    endpoint: endpoint,
                    body: requestBody);

                // Set extended timeout for iterative list generation (5 minutes)
                // The while loop may require multiple AI calls, each taking 20-40 seconds
                request.TimeoutSeconds = DefaultTimeoutSeconds;

                while (allItems.Count < count && iteration < maxIterations)
                {
                    iteration++;
                    var stillNeeded = count - allItems.Count;
                    Debug.WriteLine($"[ListTools] Iteration {iteration}: Need {stillNeeded} more items (have {allItems.Count}/{count})");

                    // 2. Execute the AIRequestCall
                    result = await request.Exec().ConfigureAwait(false);

                    // Accumulate metrics from this iteration
                    if (result.Metrics != null)
                    {
                        accumulatedMetrics.Combine(result.Metrics);
                        Debug.WriteLine($"[ListTools] Iteration {iteration} metrics: Tokens In={result.Metrics.InputTokensPrompt}, Out={result.Metrics.OutputTokensGeneration}, Time={result.Metrics.CompletionTime:F2}s");
                    }

                    // Accumulate messages from this iteration
                    if (result.Messages != null && result.Messages.Count > 0)
                    {
                        allMessages.AddRange(result.Messages);
                    }

                    // Check if first attempt was truncated due to token limits (check BEFORE success check)
                    if (iteration == 1 && result.Metrics?.FinishReason?.Equals("length", StringComparison.OrdinalIgnoreCase) == true)
                    {
                        Debug.WriteLine($"[ListTools] First attempt truncated (finish_reason=length). Attempting to parse partial response and restart with batch strategy.");

                        // Try to parse and save any valid items from the truncated response
                        var assistantTruncated = result.Body.GetLastInteraction(AIAgent.Assistant) as AIInteractionText;
                        var responseTruncated = assistantTruncated?.Content ?? string.Empty;
                        Debug.WriteLine($"[ListTools] Truncated response length: {responseTruncated.Length} chars");

                        List<string> partialItems = new List<string>();
                        if (!string.IsNullOrWhiteSpace(responseTruncated))
                        {
                            try
                            {
                                // Try to unwrap and parse partial response
                                responseTruncated = UnwrapResponseIfNeeded(responseTruncated);
                                partialItems = AIResponseParser.ParseStringArrayFromResponse(responseTruncated);
                                Debug.WriteLine($"[ListTools] Successfully parsed {partialItems.Count} items from truncated response");
                            }
                            catch (Exception ex)
                            {
                                Debug.WriteLine($"[ListTools] Could not parse truncated response: {ex.Message}. Starting fresh.");
                                partialItems.Clear();
                            }
                        }

                        // Add any valid items we got
                        if (partialItems.Count > 0)
                        {
                            allItems.AddRange(partialItems);
                            Debug.WriteLine($"[ListTools] Saved {partialItems.Count} items from truncated response. Total now: {allItems.Count}");
                        }

                        // If we already have all items needed, we're done
                        if (allItems.Count >= count)
                        {
                            TrimToCount(allItems, count);
                            Debug.WriteLine($"[ListTools] Target count {count} reached from truncated response");
                            break;
                        }

                        // Calculate a safe batch size based on what was received
                        int receivedCount = partialItems.Count;
                        currentBatchSize = CalculateBatchSize(receivedCount, count);
                        Debug.WriteLine($"[ListTools] Parsed {receivedCount} items from truncated response. Setting batch size to {currentBatchSize} for remaining items");

                        // Start a new conversation with a batch-based approach
                        int batchStart = allItems.Count + 1;
                        int batchEnd = Math.Min(allItems.Count + currentBatchSize, count);
                        int batchCount = batchEnd - batchStart + 1;

                        var batchUserPrompt = $"The full list will have {count} items. Generate items {batchStart} through {batchEnd} ({batchCount} items) based on this prompt: \"{prompt}\"\n\n" +
                                            $"Return only the JSON array of these {batchCount} items.";

                        requestBody = AIBodyBuilder.Create()
                            .WithJsonOutputSchema(this.listJsonSchema)
                            .AddSystem(this.systemPrompt)
                            .AddUser(batchUserPrompt)
                            .WithContextFilter(contextFilter)
                            .Build();

                        request.Body = requestBody;

                        // Reset iteration counter to restart the loop with new strategy
                        Debug.WriteLine($"[ListTools] Restarting with batch strategy: requesting {batchCount} items (range {batchStart}-{batchEnd})");
                        iteration = 0;
                        continue;
                    }

                    // Check for general failure (non-truncation errors)
                    if (!result.Success)
                    {
                        Debug.WriteLine($"[ListTools] Request failed: {result.Messages?.FirstOrDefault()?.Message ?? "Unknown error"}");
                        output.Messages = result.Messages;
                        return output;
                    }

                    // 3. Parse the output and check if count is reached
                    var assistant = result.Body.GetLastInteraction(AIAgent.Assistant) as AIInteractionText;
                    var response = assistant?.Content ?? string.Empty;
                    Debug.WriteLine($"[ListTools] AI response: {response}");

                    if (string.IsNullOrWhiteSpace(response))
                    {
                        // Propagate runtime messages from the AI call (e.g., token length truncation)
                        if (result != null && result.Messages != null)
                        {
                            output.Messages = result.Messages;
                        }

                        // Standardize the tool error so it appears with Tool origin alongside the propagated messages
                        output.CreateToolError("Empty response from AI assistant.");
                        return output;
                    }

                    // Unwrap cases where providers return an object with an 'items' (or 'list') array
                    response = UnwrapResponseIfNeeded(response);

                    // Parse JSON array of strings
                    List<string> newItems;
                    try
                    {
                        newItems = AIResponseParser.ParseStringArrayFromResponse(response);
                    }
                    catch (Exception parseEx)
                    {
                        Debug.WriteLine($"[ListTools] Error parsing response in iteration {iteration}: {parseEx.Message}");
                        Debug.WriteLine($"[ListTools] Raw response: {response}");

                        // If we have some items already, return what we have
                        if (allItems.Count > 0)
                        {
                            Debug.WriteLine($"[ListTools] Returning partial list with {allItems.Count} items due to parsing error");
                            Debug.WriteLine($"[ListTools] Partial accumulated metrics: Tokens In={accumulatedMetrics.InputTokensPrompt}, Out={accumulatedMetrics.OutputTokensGeneration}, Time={accumulatedMetrics.CompletionTime:F2}s");

                            var partialResult = this.CreateToolResultWithEnvelope(allItems, providerName, modelName, toolInfo?.Id);
                            var partialBody = AIBodyBuilder.Create()
                                .AddToolResult(partialResult, id: toolInfo?.Id, name: this.toolName, metrics: accumulatedMetrics, messages: allMessages)
                                .Build();

                            output.CreateSuccess(partialBody);
                            return output;
                        }

                        // Otherwise, return the error
                        output.CreateError($"Error parsing AI response: {parseEx.Message}");
                        return output;
                    }

                    Debug.WriteLine($"[ListTools] Iteration {iteration} generated {newItems.Count} items: {string.Join(", ", newItems)}");

                    // Add new items to our collection
                    foreach (var item in newItems)
                    {
                        allItems.Add(item);
                    }

                    // If we have enough items, trim to exact count and return
                    if (allItems.Count >= count)
                    {
                        TrimToCount(allItems, count);
                        Debug.WriteLine($"[ListTools] Target count {count} reached after {iteration} iterations");
                        break;
                    }

                    // 4. If count not reached, build conversation for next iteration
                    // Use the request from the last return and add assistant + user interactions
                    stillNeeded = count - allItems.Count;
                    Debug.WriteLine($"[ListTools] Requesting {stillNeeded} more items in next iteration");

                    // Check if this iteration was also truncated - reduce batch size further
                    if (result.Metrics?.FinishReason?.Equals("length", StringComparison.OrdinalIgnoreCase) == true)
                    {
                        int itemsReceived = newItems.Count;
                        currentBatchSize = Math.Max(MinReducedBatchSize, itemsReceived / 2);
                        Debug.WriteLine($"[ListTools] Iteration {iteration} truncated. Reducing batch size to {currentBatchSize}");
                    }

                    // Calculate next batch size
                    int nextBatchSize = Math.Min(stillNeeded, currentBatchSize);
                    int nextStart = allItems.Count + 1;
                    int nextEnd = allItems.Count + nextBatchSize;

                    // Add the assistant response and follow-up user message to the conversation immutably
                    var followUpMessage = $"Good! I now have {allItems.Count} items. The full list will have {count} items total. " +
                                         $"Generate items {nextStart} through {nextEnd} ({nextBatchSize} items) based on the original prompt: \"{prompt}\"\n\n" +
                                         $"Return only the JSON array of the next {nextBatchSize} items.";

                    request.Body = AIBodyBuilder.FromImmutable(request.Body)
                        .AddAssistant(response)
                        .AddUser(followUpMessage)
                        .Build();
                }

                if (allItems.Count == 0)
                {
                    // Add diagnostic info about what happened
                    var diagMessage = $"AI failed to generate any valid items after {iteration} iterations. ";
                    if (result?.Metrics?.FinishReason != null)
                    {
                        diagMessage += $"Last finish reason: {result.Metrics.FinishReason}. ";
                    }

                    diagMessage += "This may indicate the response was truncated and retry logic failed.";
                    Debug.WriteLine($"[ListTools] {diagMessage}");
                    output.CreateError(diagMessage);
                    return output;
                }

                // Final safety check: trim list if it's longer than requested
                if (allItems.Count > count)
                {
                    Debug.WriteLine($"[ListTools] Trimming final list from {allItems.Count} to {count} items");
                    TrimToCount(allItems, count);
                }

                Debug.WriteLine($"[ListTools] Final result: {allItems.Count} items generated: {string.Join(", ", allItems)}");
                Debug.WriteLine($"[ListTools] Total accumulated metrics: Tokens In={accumulatedMetrics.InputTokensPrompt}, Out={accumulatedMetrics.OutputTokensGeneration}, Time={accumulatedMetrics.CompletionTime:F2}s, Iterations={iteration}");

                // Success case - use accumulated metrics from all iterations
                var toolResult = this.CreateToolResultWithEnvelope(allItems, providerName, modelName, toolInfo?.Id);
                var toolBody = AIBodyBuilder.Create()
                    .AddToolResult(toolResult, id: toolInfo?.Id, name: this.toolName, metrics: accumulatedMetrics, messages: allMessages)
                    .Build();

                output.CreateSuccess(toolBody);
                return output;
            }
            catch (Exception ex)
            {
                Debug.WriteLine($"[ListTools] Error in GenerateList: {ex.Message}");

                output.CreateError($"Error: {ex.Message}");
                return output;
            }
        }
    }
}
